{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-25T00:03:50.463921Z",
     "start_time": "2025-07-25T00:03:44.507490Z"
    }
   },
   "source": [
    "import dspy\n",
    "from datasets import load_dataset, Dataset\n",
    "from dspy.datasets import DataLoader\n",
    "from src.logger import logger\n",
    "import json\n",
    "import random\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import openai\n",
    "from typing import Any\n",
    "import jellyfish\n",
    "import gc\n",
    "from itertools import combinations\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import re\n",
    "\n",
    "# PEFT (parameter-efficient fine-tuning) methods enable efficient adaptation of large pretrained models\n",
    "from peft import LoraConfig,  get_peft_model, prepare_model_for_kbit_training"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lausena/developer/research/gdls/dspy_experiments/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T00:03:50.624997Z",
     "start_time": "2025-07-25T00:03:50.477261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hugging face login\n",
    "login(token=os.getenv(\"HUGGING_FACE_TOKEN\"))"
   ],
   "id": "295471065591be08",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T00:03:50.702354Z",
     "start_time": "2025-07-25T00:03:50.699650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_ = load_dotenv(find_dotenv())\n",
    "os.environ['OPENAI_API_KEY'] = os.environ['OPENAI_API_KEY']\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ],
   "id": "72e45cef3d16a628",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T00:03:51.068585Z",
     "start_time": "2025-07-25T00:03:50.772122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import tempfile\n",
    "from datasets import load_dataset\n",
    "from typing import Dict, Any, List\n",
    "import dspy\n",
    "\n",
    "def load_conll_dataset() -> dict:\n",
    "    \"\"\"\n",
    "    Loads the CoNLL-2003 dataset into train, validation, and test splits.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dataset splits with keys 'train', 'validation', and 'test'.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # Use a temporary Hugging Face cache directory for compatibility with certain hosted notebook\n",
    "        # environments that don't support the default Hugging Face cache directory\n",
    "        os.environ[\"HF_DATASETS_CACHE\"] = temp_dir\n",
    "        return load_dataset(\"conll2003\", trust_remote_code=True, token=os.getenv(\"HUGGING_FACE_TOKEN\"))\n",
    "\n",
    "def extract_people_entities(data_row: dict[str, Any]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extracts entities referring to people from a row of the CoNLL-2003 dataset.\n",
    "\n",
    "    Args:\n",
    "        data_row (dict[str, Any]): A row from the dataset containing tokens and NER tags.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of tokens tagged as people.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        token\n",
    "        for token, ner_tag in zip(data_row[\"tokens\"], data_row[\"ner_tags\"])\n",
    "        if ner_tag in (1, 2)  # CoNLL entity codes 1 and 2 refer to people\n",
    "    ]\n",
    "\n",
    "def prepare_dataset(data_split, start: int, end: int) -> list[dspy.Example]:\n",
    "    \"\"\"\n",
    "    Prepares a sliced dataset split for use with DSPy.\n",
    "\n",
    "    Args:\n",
    "        data_split: The dataset split (e.g., train or test).\n",
    "        start (int): Starting index of the slice.\n",
    "        end (int): Ending index of the slice.\n",
    "\n",
    "    Returns:\n",
    "        list[dspy.Example]: List of DSPy Examples with tokens and expected labels.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        dspy.Example(\n",
    "            tokens=row[\"tokens\"],\n",
    "            expected_extracted_people=extract_people_entities(row)\n",
    "        ).with_inputs(\"tokens\")\n",
    "        for row in data_split.select(range(start, end))\n",
    "    ]\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_conll_dataset()\n",
    "\n",
    "# Prepare the training and test sets\n",
    "train_set = prepare_dataset(dataset[\"train\"], 0, 50)\n",
    "test_set = prepare_dataset(dataset[\"test\"], 0, 200)"
   ],
   "id": "8fd6ee8cd5badf17",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'conll2003' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "Using the latest cached version of the dataset since conll2003 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'conll2003' at /Users/lausena/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98 (last modified on Tue Jul 22 19:54:00 2025).\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T23:44:19.917886Z",
     "start_time": "2025-07-24T23:30:49.812184Z"
    }
   },
   "cell_type": "code",
   "source": "train_set",
   "id": "fe0ff068c420a18f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example({'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Peter', 'Blackburn'], 'expected_extracted_people': ['Peter', 'Blackburn']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['BRUSSELS', '1996-08-22'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['The', 'European', 'Commission', 'said', 'on', 'Thursday', 'it', 'disagreed', 'with', 'German', 'advice', 'to', 'consumers', 'to', 'shun', 'British', 'lamb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Germany', \"'s\", 'representative', 'to', 'the', 'European', 'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.'], 'expected_extracted_people': ['Werner', 'Zwingmann']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['\"', 'We', 'do', \"n't\", 'support', 'any', 'such', 'recommendation', 'because', 'we', 'do', \"n't\", 'see', 'any', 'grounds', 'for', 'it', ',', '\"', 'the', 'Commission', \"'s\", 'chief', 'spokesman', 'Nikolaus', 'van', 'der', 'Pas', 'told', 'a', 'news', 'briefing', '.'], 'expected_extracted_people': ['Nikolaus', 'van', 'der', 'Pas']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['He', 'said', 'further', 'scientific', 'study', 'was', 'required', 'and', 'if', 'it', 'was', 'found', 'that', 'action', 'was', 'needed', 'it', 'should', 'be', 'taken', 'by', 'the', 'European', 'Union', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['He', 'said', 'a', 'proposal', 'last', 'month', 'by', 'EU', 'Farm', 'Commissioner', 'Franz', 'Fischler', 'to', 'ban', 'sheep', 'brains', ',', 'spleens', 'and', 'spinal', 'cords', 'from', 'the', 'human', 'and', 'animal', 'food', 'chains', 'was', 'a', 'highly', 'specific', 'and', 'precautionary', 'move', 'to', 'protect', 'human', 'health', '.'], 'expected_extracted_people': ['Franz', 'Fischler']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Fischler', 'proposed', 'EU-wide', 'measures', 'after', 'reports', 'from', 'Britain', 'and', 'France', 'that', 'under', 'laboratory', 'conditions', 'sheep', 'could', 'contract', 'Bovine', 'Spongiform', 'Encephalopathy', '(', 'BSE', ')', '--', 'mad', 'cow', 'disease', '.'], 'expected_extracted_people': ['Fischler']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['But', 'Fischler', 'agreed', 'to', 'review', 'his', 'proposal', 'after', 'the', 'EU', \"'s\", 'standing', 'veterinary', 'committee', ',', 'mational', 'animal', 'health', 'officials', ',', 'questioned', 'if', 'such', 'action', 'was', 'justified', 'as', 'there', 'was', 'only', 'a', 'slight', 'risk', 'to', 'human', 'health', '.'], 'expected_extracted_people': ['Fischler']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Spanish', 'Farm', 'Minister', 'Loyola', 'de', 'Palacio', 'had', 'earlier', 'accused', 'Fischler', 'at', 'an', 'EU', 'farm', 'ministers', \"'\", 'meeting', 'of', 'causing', 'unjustified', 'alarm', 'through', '\"', 'dangerous', 'generalisation', '.', '\"'], 'expected_extracted_people': ['Loyola', 'de', 'Palacio', 'Fischler']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Only', 'France', 'and', 'Britain', 'backed', 'Fischler', \"'s\", 'proposal', '.'], 'expected_extracted_people': ['Fischler']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['The', 'EU', \"'s\", 'scientific', 'veterinary', 'and', 'multidisciplinary', 'committees', 'are', 'due', 'to', 're-examine', 'the', 'issue', 'early', 'next', 'month', 'and', 'make', 'recommendations', 'to', 'the', 'senior', 'veterinary', 'officials', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Sheep', 'have', 'long', 'been', 'known', 'to', 'contract', 'scrapie', ',', 'a', 'brain-wasting', 'disease', 'similar', 'to', 'BSE', 'which', 'is', 'believed', 'to', 'have', 'been', 'transferred', 'to', 'cattle', 'through', 'feed', 'containing', 'animal', 'waste', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['British', 'farmers', 'denied', 'on', 'Thursday', 'there', 'was', 'any', 'danger', 'to', 'human', 'health', 'from', 'their', 'sheep', ',', 'but', 'expressed', 'concern', 'that', 'German', 'government', 'advice', 'to', 'consumers', 'to', 'avoid', 'British', 'lamb', 'might', 'influence', 'consumers', 'across', 'Europe', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['\"', 'What', 'we', 'have', 'to', 'be', 'extremely', 'careful', 'of', 'is', 'how', 'other', 'countries', 'are', 'going', 'to', 'take', 'Germany', \"'s\", 'lead', ',', '\"', 'Welsh', 'National', 'Farmers', \"'\", 'Union', '(', 'NFU', ')', 'chairman', 'John', 'Lloyd', 'Jones', 'said', 'on', 'BBC', 'radio', '.'], 'expected_extracted_people': ['John', 'Lloyd', 'Jones']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Bonn', 'has', 'led', 'efforts', 'to', 'protect', 'public', 'health', 'after', 'consumer', 'confidence', 'collapsed', 'in', 'March', 'after', 'a', 'British', 'report', 'suggested', 'humans', 'could', 'contract', 'an', 'illness', 'similar', 'to', 'mad', 'cow', 'disease', 'by', 'eating', 'contaminated', 'beef', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Germany', 'imported', '47,600', 'sheep', 'from', 'Britain', 'last', 'year', ',', 'nearly', 'half', 'of', 'total', 'imports', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['It', 'brought', 'in', '4,275', 'tonnes', 'of', 'British', 'mutton', ',', 'some', '10', 'percent', 'of', 'overall', 'imports', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Rare', 'Hendrix', 'song', 'draft', 'sells', 'for', 'almost', '$', '17,000', '.'], 'expected_extracted_people': ['Hendrix']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['LONDON', '1996-08-22'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['A', 'rare', 'early', 'handwritten', 'draft', 'of', 'a', 'song', 'by', 'U.S.', 'guitar', 'legend', 'Jimi', 'Hendrix', 'was', 'sold', 'for', 'almost', '$', '17,000', 'on', 'Thursday', 'at', 'an', 'auction', 'of', 'some', 'of', 'the', 'late', 'musician', \"'s\", 'favourite', 'possessions', '.'], 'expected_extracted_people': ['Jimi', 'Hendrix']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['A', 'Florida', 'restaurant', 'paid', '10,925', 'pounds', '(', '$', '16,935', ')', 'for', 'the', 'draft', 'of', '\"', 'Ai', \"n't\", 'no', 'telling', '\"', ',', 'which', 'Hendrix', 'penned', 'on', 'a', 'piece', 'of', 'London', 'hotel', 'stationery', 'in', 'late', '1966', '.'], 'expected_extracted_people': ['Hendrix']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['At', 'the', 'end', 'of', 'a', 'January', '1967', 'concert', 'in', 'the', 'English', 'city', 'of', 'Nottingham', 'he', 'threw', 'the', 'sheet', 'of', 'paper', 'into', 'the', 'audience', ',', 'where', 'it', 'was', 'retrieved', 'by', 'a', 'fan', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Buyers', 'also', 'snapped', 'up', '16', 'other', 'items', 'that', 'were', 'put', 'up', 'for', 'auction', 'by', 'Hendrix', \"'s\", 'former', 'girlfriend', 'Kathy', 'Etchingham', ',', 'who', 'lived', 'with', 'him', 'from', '1966', 'to', '1969', '.'], 'expected_extracted_people': ['Hendrix', 'Kathy', 'Etchingham']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['They', 'included', 'a', 'black', 'lacquer', 'and', 'mother', 'of', 'pearl', 'inlaid', 'box', 'used', 'by', 'Hendrix', 'to', 'store', 'his', 'drugs', ',', 'which', 'an', 'anonymous', 'Australian', 'purchaser', 'bought', 'for', '5,060', 'pounds', '(', '$', '7,845', ')', '.'], 'expected_extracted_people': ['Hendrix']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['The', 'guitarist', 'died', 'of', 'a', 'drugs', 'overdose', 'in', '1970', 'aged', '27', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['China', 'says', 'Taiwan', 'spoils', 'atmosphere', 'for', 'talks', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['BEIJING', '1996-08-22'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['China', 'on', 'Thursday', 'accused', 'Taipei', 'of', 'spoiling', 'the', 'atmosphere', 'for', 'a', 'resumption', 'of', 'talks', 'across', 'the', 'Taiwan', 'Strait', 'with', 'a', 'visit', 'to', 'Ukraine', 'by', 'Taiwanese', 'Vice', 'President', 'Lien', 'Chan', 'this', 'week', 'that', 'infuriated', 'Beijing', '.'], 'expected_extracted_people': ['Lien', 'Chan']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Speaking', 'only', 'hours', 'after', 'Chinese', 'state', 'media', 'said', 'the', 'time', 'was', 'right', 'to', 'engage', 'in', 'political', 'talks', 'with', 'Taiwan', ',', 'Foreign', 'Ministry', 'spokesman', 'Shen', 'Guofang', 'told', 'Reuters', ':', '\"', 'The', 'necessary', 'atmosphere', 'for', 'the', 'opening', 'of', 'the', 'talks', 'has', 'been', 'disrupted', 'by', 'the', 'Taiwan', 'authorities', '.', '\"'], 'expected_extracted_people': ['Shen', 'Guofang']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['State', 'media', 'quoted', 'China', \"'s\", 'top', 'negotiator', 'with', 'Taipei', ',', 'Tang', 'Shubei', ',', 'as', 'telling', 'a', 'visiting', 'group', 'from', 'Taiwan', 'on', 'Wednesday', 'that', 'it', 'was', 'time', 'for', 'the', 'rivals', 'to', 'hold', 'political', 'talks', '.'], 'expected_extracted_people': ['Tang', 'Shubei']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['\"', 'Now', 'is', 'the', 'time', 'for', 'the', 'two', 'sides', 'to', 'engage', 'in', 'political', 'talks', '...'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['that', 'is', 'to', 'end', 'the', 'state', 'of', 'hostility', ',', '\"', 'Thursday', \"'s\", 'overseas', 'edition', 'of', 'the', 'People', \"'s\", 'Daily', 'quoted', 'Tang', 'as', 'saying', '.'], 'expected_extracted_people': ['Tang']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['The', 'foreign', 'ministry', \"'s\", 'Shen', 'told', 'Reuters', 'Television', 'in', 'an', 'interview', 'he', 'had', 'read', 'reports', 'of', 'Tang', \"'s\", 'comments', 'but', 'gave', 'no', 'details', 'of', 'why', 'the', 'negotiator', 'had', 'considered', 'the', 'time', 'right', 'for', 'talks', 'with', 'Taiwan', ',', 'which', 'Beijing', 'considers', 'a', 'renegade', 'province', '.'], 'expected_extracted_people': ['Tang']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['China', ',', 'which', 'has', 'long', 'opposed', 'all', 'Taipei', 'efforts', 'to', 'gain', 'greater', 'international', 'recognition', ',', 'was', 'infuriated', 'by', 'a', 'visit', 'to', 'Ukraine', 'this', 'week', 'by', 'Taiwanese', 'Vice', 'President', 'Lien', '.'], 'expected_extracted_people': ['Lien']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['China', 'says', 'time', 'right', 'for', 'Taiwan', 'talks', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['BEIJING', '1996-08-22'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['China', 'has', 'said', 'it', 'was', 'time', 'for', 'political', 'talks', 'with', 'Taiwan', 'and', 'that', 'the', 'rival', 'island', 'should', 'take', 'practical', 'steps', 'towards', 'that', 'goal', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Consultations', 'should', 'be', 'held', 'to', 'set', 'the', 'time', 'and', 'format', 'of', 'the', 'talks', ',', 'the', 'official', 'Xinhua', 'news', 'agency', 'quoted', 'Tang', 'Shubei', ',', 'executive', 'vice', 'chairman', 'of', 'the', 'Association', 'for', 'Relations', 'Across', 'the', 'Taiwan', 'Straits', ',', 'as', 'saying', 'late', 'on', 'Wednesday', '.'], 'expected_extracted_people': ['Tang', 'Shubei']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['German', 'July', 'car', 'registrations', 'up', '14.2', 'pct', 'yr', '/', 'yr', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['FRANKFURT', '1996-08-22'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['German', 'first-time', 'registrations', 'of', 'motor', 'vehicles', 'jumped', '14.2', 'percent', 'in', 'July', 'this', 'year', 'from', 'the', 'year-earlier', 'period', ',', 'the', 'Federal', 'office', 'for', 'motor', 'vehicles', 'said', 'on', 'Thursday', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['The', 'office', 'said', '356,725', 'new', 'cars', 'were', 'registered', 'in', 'July', '1996', '--', '304,850', 'passenger', 'cars', 'and', '15,613', 'trucks', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['The', 'figures', 'represent', 'a', '13.6', 'percent', 'increase', 'for', 'passenger', 'cars', 'and', 'a', '2.2', 'percent', 'decline', 'for', 'trucks', 'from', 'July', '1995', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Motor-bike', 'registration', 'rose', '32.7', 'percent', 'in', 'the', 'period', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['The', 'growth', 'was', 'partly', 'due', 'to', 'an', 'increased', 'number', 'of', 'Germans', 'buying', 'German', 'cars', 'abroad', ',', 'while', 'manufacturers', 'said', 'that', 'domestic', 'demand', 'was', 'weak', ',', 'the', 'federal', 'office', 'said', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Almost', 'all', 'German', 'car', 'manufacturers', 'posted', 'gains', 'in', 'registration', 'numbers', 'in', 'the', 'period', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Volkswagen', 'AG', 'won', '77,719', 'registrations', ',', 'slightly', 'more', 'than', 'a', 'quarter', 'of', 'the', 'total', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T23:44:19.918294Z",
     "start_time": "2025-07-23T12:50:24.643465Z"
    }
   },
   "cell_type": "code",
   "source": "test_set",
   "id": "464f85b562a86a1f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example({'tokens': ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.'], 'expected_extracted_people': ['CHINA']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Nadim', 'Ladki'], 'expected_extracted_people': ['Nadim', 'Ladki']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['AL-AIN', ',', 'United', 'Arab', 'Emirates', '1996-12-06'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Japan', 'began', 'the', 'defence', 'of', 'their', 'Asian', 'Cup', 'title', 'with', 'a', 'lucky', '2-1', 'win', 'against', 'Syria', 'in', 'a', 'Group', 'C', 'championship', 'match', 'on', 'Friday', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['But', 'China', 'saw', 'their', 'luck', 'desert', 'them', 'in', 'the', 'second', 'match', 'of', 'the', 'group', ',', 'crashing', 'to', 'a', 'surprise', '2-0', 'defeat', 'to', 'newcomers', 'Uzbekistan', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['China', 'controlled', 'most', 'of', 'the', 'match', 'and', 'saw', 'several', 'chances', 'missed', 'until', 'the', '78th', 'minute', 'when', 'Uzbek', 'striker', 'Igor', 'Shkvyrin', 'took', 'advantage', 'of', 'a', 'misdirected', 'defensive', 'header', 'to', 'lob', 'the', 'ball', 'over', 'the', 'advancing', 'Chinese', 'keeper', 'and', 'into', 'an', 'empty', 'net', '.'], 'expected_extracted_people': ['Igor', 'Shkvyrin']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Oleg', 'Shatskiku', 'made', 'sure', 'of', 'the', 'win', 'in', 'injury', 'time', ',', 'hitting', 'an', 'unstoppable', 'left', 'foot', 'shot', 'from', 'just', 'outside', 'the', 'area', '.'], 'expected_extracted_people': ['Oleg', 'Shatskiku']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['The', 'former', 'Soviet', 'republic', 'was', 'playing', 'in', 'an', 'Asian', 'Cup', 'finals', 'tie', 'for', 'the', 'first', 'time', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Despite', 'winning', 'the', 'Asian', 'Games', 'title', 'two', 'years', 'ago', ',', 'Uzbekistan', 'are', 'in', 'the', 'finals', 'as', 'outsiders', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Two', 'goals', 'from', 'defensive', 'errors', 'in', 'the', 'last', 'six', 'minutes', 'allowed', 'Japan', 'to', 'come', 'from', 'behind', 'and', 'collect', 'all', 'three', 'points', 'from', 'their', 'opening', 'meeting', 'against', 'Syria', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Takuya', 'Takagi', 'scored', 'the', 'winner', 'in', 'the', '88th', 'minute', ',', 'rising', 'to', 'head', 'a', 'Hiroshige', 'Yanagimoto', 'cross', 'towards', 'the', 'Syrian', 'goal', 'which', 'goalkeeper', 'Salem', 'Bitar', 'appeared', 'to', 'have', 'covered', 'but', 'then', 'allowed', 'to', 'slip', 'into', 'the', 'net', '.'], 'expected_extracted_people': ['Takuya', 'Takagi', 'Hiroshige', 'Yanagimoto', 'Salem', 'Bitar']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['It', 'was', 'the', 'second', 'costly', 'blunder', 'by', 'Syria', 'in', 'four', 'minutes', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Defender', 'Hassan', 'Abbas', 'rose', 'to', 'intercept', 'a', 'long', 'ball', 'into', 'the', 'area', 'in', 'the', '84th', 'minute', 'but', 'only', 'managed', 'to', 'divert', 'it', 'into', 'the', 'top', 'corner', 'of', 'Bitar', \"'s\", 'goal', '.'], 'expected_extracted_people': ['Hassan', 'Abbas', 'Bitar']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Nader', 'Jokhadar', 'had', 'given', 'Syria', 'the', 'lead', 'with', 'a', 'well-struck', 'header', 'in', 'the', 'seventh', 'minute', '.'], 'expected_extracted_people': ['Nader', 'Jokhadar']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Japan', 'then', 'laid', 'siege', 'to', 'the', 'Syrian', 'penalty', 'area', 'for', 'most', 'of', 'the', 'game', 'but', 'rarely', 'breached', 'the', 'Syrian', 'defence', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Bitar', 'pulled', 'off', 'fine', 'saves', 'whenever', 'they', 'did', '.'], 'expected_extracted_people': ['Bitar']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Japan', 'coach', 'Shu', 'Kamo', 'said', ':', \"'\", \"'\", 'The', 'Syrian', 'own', 'goal', 'proved', 'lucky', 'for', 'us', '.'], 'expected_extracted_people': ['Shu', 'Kamo']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['The', 'Syrians', 'scored', 'early', 'and', 'then', 'played', 'defensively', 'and', 'adopted', 'long', 'balls', 'which', 'made', 'it', 'hard', 'for', 'us', '.', \"'\"], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': [\"'\"], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Japan', ',', 'co-hosts', 'of', 'the', 'World', 'Cup', 'in', '2002', 'and', 'ranked', '20th', 'in', 'the', 'world', 'by', 'FIFA', ',', 'are', 'favourites', 'to', 'regain', 'their', 'title', 'here', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Hosts', 'UAE', 'play', 'Kuwait', 'and', 'South', 'Korea', 'take', 'on', 'Indonesia', 'on', 'Saturday', 'in', 'Group', 'A', 'matches', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['All', 'four', 'teams', 'are', 'level', 'with', 'one', 'point', 'each', 'from', 'one', 'game', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['RUGBY', 'UNION', '-', 'CUTTITTA', 'BACK', 'FOR', 'ITALY', 'AFTER', 'A', 'YEAR', '.'], 'expected_extracted_people': ['CUTTITTA']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['ROME', '1996-12-06'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Italy', 'recalled', 'Marcello', 'Cuttitta'], 'expected_extracted_people': ['Marcello', 'Cuttitta']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['on', 'Friday', 'for', 'their', 'friendly', 'against', 'Scotland', 'at', 'Murrayfield', 'more', 'than', 'a', 'year', 'after', 'the', '30-year-old', 'wing', 'announced', 'he', 'was', 'retiring', 'following', 'differences', 'over', 'selection', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Cuttitta', ',', 'who', 'trainer', 'George', 'Coste', 'said', 'was', 'certain', 'to', 'play', 'on', 'Saturday', 'week', ',', 'was', 'named', 'in', 'a', '21-man', 'squad', 'lacking', 'only', 'two', 'of', 'the', 'team', 'beaten', '54-21', 'by', 'England', 'at', 'Twickenham', 'last', 'month', '.'], 'expected_extracted_people': ['Cuttitta', 'George', 'Coste']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Stefano', 'Bordon', 'is', 'out', 'through', 'illness', 'and', 'Coste', 'said', 'he', 'had', 'dropped', 'back', 'row', 'Corrado', 'Covi', ',', 'who', 'had', 'been', 'recalled', 'for', 'the', 'England', 'game', 'after', 'five', 'years', 'out', 'of', 'the', 'national', 'team', '.'], 'expected_extracted_people': ['Stefano', 'Bordon', 'Coste', 'Corrado', 'Covi']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Cuttitta', 'announced', 'his', 'retirement', 'after', 'the', '1995', 'World', 'Cup', ',', 'where', 'he', 'took', 'issue', 'with', 'being', 'dropped', 'from', 'the', 'Italy', 'side', 'that', 'faced', 'England', 'in', 'the', 'pool', 'stages', '.'], 'expected_extracted_people': ['Cuttitta']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Coste', 'said', 'he', 'had', 'approached', 'the', 'player', 'two', 'months', 'ago', 'about', 'a', 'comeback', '.'], 'expected_extracted_people': ['Coste']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['\"', 'He', 'ended', 'the', 'World', 'Cup', 'on', 'the', 'wrong', 'note', ',', '\"', 'Coste', 'said', '.'], 'expected_extracted_people': ['Coste']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['\"', 'I', 'thought', 'it', 'would', 'be', 'useful', 'to', 'have', 'him', 'back', 'and', 'he', 'said', 'he', 'would', 'be', 'available', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['I', 'think', 'now', 'is', 'the', 'right', 'time', 'for', 'him', 'to', 'return', '.', '\"'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Squad', ':', 'Javier', 'Pertile', ',', 'Paolo', 'Vaccari', ',', 'Marcello', 'Cuttitta', ',', 'Ivan', 'Francescato', ',', 'Leandro', 'Manteri', ',', 'Diego', 'Dominguez', ',', 'Francesco', 'Mazzariol', ',', 'Alessandro', 'Troncon', ',', 'Orazio', 'Arancio', ',', 'Andrea', 'Sgorlon', ',', 'Massimo', 'Giovanelli', ',', 'Carlo', 'Checchinato', ',', 'Walter', 'Cristofoletto', ',', 'Franco', 'Properzi', 'Curti', ',', 'Carlo', 'Orlandi', ',', 'Massimo', 'Cuttitta', ',', 'Giambatista', 'Croci', ',', 'Gianluca', 'Guidi', ',', 'Nicola', 'Mazzucato', ',', 'Alessandro', 'Moscardi', ',', 'Andrea', 'Castellani', '.'], 'expected_extracted_people': ['Javier', 'Pertile', 'Paolo', 'Vaccari', 'Marcello', 'Cuttitta', 'Ivan', 'Francescato', 'Leandro', 'Manteri', 'Diego', 'Dominguez', 'Francesco', 'Mazzariol', 'Alessandro', 'Troncon', 'Orazio', 'Arancio', 'Andrea', 'Sgorlon', 'Massimo', 'Giovanelli', 'Carlo', 'Checchinato', 'Walter', 'Cristofoletto', 'Franco', 'Properzi', 'Curti', 'Carlo', 'Orlandi', 'Massimo', 'Cuttitta', 'Giambatista', 'Croci', 'Gianluca', 'Guidi', 'Nicola', 'Mazzucato', 'Alessandro', 'Moscardi', 'Andrea', 'Castellani']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['SOCCER', '-', 'LATE', 'GOALS', 'GIVE', 'JAPAN', 'WIN', 'OVER', 'SYRIA', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['AL-AIN', ',', 'United', 'Arab', 'Emirates', '1996-12-06'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Two', 'goals', 'in', 'the', 'last', 'six', 'minutes', 'gave', 'holders', 'Japan', 'an', 'uninspiring', '2-1', 'Asian', 'Cup', 'victory', 'over', 'Syria', 'on', 'Friday', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Takuya', 'Takagi', 'headed', 'the', 'winner', 'in', 'the', '88th', 'minute', 'of', 'the', 'group', 'C', 'game', 'after', 'goalkeeper', 'Salem', 'Bitar', 'spoiled', 'a', 'mistake-free', 'display', 'by', 'allowing', 'the', 'ball', 'to', 'slip', 'under', 'his', 'body', '.'], 'expected_extracted_people': ['Takuya', 'Takagi', 'Salem', 'Bitar']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['It', 'was', 'the', 'second', 'Syrian', 'defensive', 'blunder', 'in', 'four', 'minutes', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Defender', 'Hassan', 'Abbas', 'rose', 'to', 'intercept', 'a', 'long', 'ball', 'into', 'the', 'area', 'in', 'the', '84th', 'minute', 'but', 'only', 'managed', 'to', 'divert', 'it', 'into', 'the', 'top', 'corner', 'of', 'Bitar', \"'s\", 'goal', '.'], 'expected_extracted_people': ['Hassan', 'Abbas', 'Bitar']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Syria', 'had', 'taken', 'the', 'lead', 'from', 'their', 'first', 'serious', 'attack', 'in', 'the', 'seventh', 'minute', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Nader', 'Jokhadar', 'headed', 'a', 'cross', 'from', 'the', 'right', 'by', 'Ammar', 'Awad', 'into', 'the', 'top', 'right', 'corner', 'of', 'Kenichi', 'Shimokawa', \"'s\", 'goal', '.'], 'expected_extracted_people': ['Nader', 'Jokhadar', 'Ammar', 'Awad', 'Kenichi', 'Shimokawa']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Japan', 'then', 'laid', 'siege', 'to', 'the', 'Syrian', 'penalty', 'area', 'and', 'had', 'a', 'goal', 'disallowed', 'for', 'offside', 'in', 'the', '16th', 'minute', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['A', 'minute', 'later', ',', 'Bitar', 'produced', 'a', 'good', 'double', 'save', ',', 'first', 'from', 'Kazuyoshi', 'Miura', \"'s\", 'header', 'and', 'then', 'blocked', 'a', 'Takagi', 'follow-up', 'shot', '.'], 'expected_extracted_people': ['Bitar', 'Kazuyoshi', 'Miura', 'Takagi']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Bitar', 'saved', 'well', 'again', 'from', 'Miura', 'in', 'the', '37th', 'minute', ',', 'parrying', 'away', 'his', 'header', 'from', 'a', 'corner', '.'], 'expected_extracted_people': ['Bitar', 'Miura']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Japan', 'started', 'the', 'second', 'half', 'brightly', 'but', 'Bitar', 'denied', 'them', 'an', 'equaliser', 'when', 'he', 'dived', 'to', 'his', 'right', 'to', 'save', 'Naoki', 'Soma', \"'s\", 'low', 'drive', 'in', 'the', '53rd', 'minute', '.'], 'expected_extracted_people': ['Bitar', 'Naoki', 'Soma']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Japan', ':', '19', '-', 'Kenichi', 'Shimokawa', ',', '2', '-', 'Hiroshige', 'Yanagimoto', ',', '3', '-', 'Naoki', 'Soma', ',', '4', '-', 'Masami', 'Ihara', ',', '5', '-', 'Norio', 'Omura', ',', '6', '-', 'Motohiro', 'Yamaguchi', ',', '8', '-', 'Masakiyo', 'Maezono', '(', '7', '-', 'Yasuto', 'Honda', '71', ')', ',', '9', '-', 'Takuya', 'Takagi', ',', '10', '-', 'Hiroshi', 'Nanami', ',', '11', '-', 'Kazuyoshi', 'Miura', ',', '15', '-', 'Hiroaki', 'Morishima', '(', '14', '-', 'Masayuki', 'Okano', '75', ')', '.'], 'expected_extracted_people': ['Kenichi', 'Shimokawa', 'Hiroshige', 'Yanagimoto', 'Naoki', 'Soma', 'Masami', 'Ihara', 'Norio', 'Omura', 'Motohiro', 'Yamaguchi', 'Masakiyo', 'Maezono', 'Yasuto', 'Honda', 'Takuya', 'Takagi', 'Hiroshi', 'Nanami', 'Kazuyoshi', 'Miura', 'Hiroaki', 'Morishima', 'Masayuki', 'Okano']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Syria', ':', '24', '-', 'Salem', 'Bitar', ',', '3', '-', 'Bachar', 'Srour', ';', '4', '-', 'Hassan', 'Abbas', ',', '5', '-', 'Tarek', 'Jabban', ',', '6', '-', 'Ammar', 'Awad', '(', '9', '-', 'Louay', 'Taleb', '69', ')', ',', '8', '-', 'Nihad', 'al-Boushi', ',', '10', '-', 'Mohammed', 'Afash', ',', '12', '-', 'Ali', 'Dib', ',', '13', '-', 'Abdul', 'Latif', 'Helou', '(', '17', '-', 'Ammar', 'Rihawiy', '46', ')', ',', '14', '-', 'Khaled', 'Zaher', ';', '16', '-', 'Nader', 'Jokhadar', '.'], 'expected_extracted_people': ['Salem', 'Bitar', 'Bachar', 'Srour', 'Hassan', 'Abbas', 'Tarek', 'Jabban', 'Ammar', 'Awad', 'Louay', 'Taleb', 'Nihad', 'al-Boushi', 'Mohammed', 'Afash', 'Ali', 'Dib', 'Abdul', 'Latif', 'Helou', 'Ammar', 'Rihawiy', 'Khaled', 'Zaher', 'Nader', 'Jokhadar']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['FREESTYLE', 'SKIING-WORLD', 'CUP', 'MOGUL', 'RESULTS', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['TIGNES', ',', 'France', '1996-12-06'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Results', 'of', 'the', 'World', 'Cup'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['freestyle', 'skiing', 'moguls', 'competition', 'on', 'Friday', ':'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Men'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['1.', 'Jesper', 'Ronnback', '(', 'Sweden', ')', '25.76', 'points'], 'expected_extracted_people': ['Jesper', 'Ronnback']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['2.', 'Andrei', 'Ivanov', '(', 'Russia', ')', '24.88'], 'expected_extracted_people': ['Andrei', 'Ivanov']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['3.', 'Ryan', 'Johnson', '(', 'Canada', ')', '24.57'], 'expected_extracted_people': ['Ryan', 'Johnson']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['4.', 'Jean-Luc', 'Brassard', '(', 'Canada', ')', '24.40'], 'expected_extracted_people': ['Jean-Luc', 'Brassard']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['5.', 'Korneilus', 'Hole', '(', 'Norway', ')', '23.92'], 'expected_extracted_people': ['Korneilus', 'Hole']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['6.', 'Jeremie', 'Collomb-Patton', '(', 'France', ')', '23.87'], 'expected_extracted_people': ['Jeremie', 'Collomb-Patton']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['7.', 'Jim', 'Moran', '(', 'U.S.', ')', '23.25'], 'expected_extracted_people': ['Jim', 'Moran']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['8.', 'Dominick', 'Gauthier', '(', 'Canada', ')', '22.73'], 'expected_extracted_people': ['Dominick', 'Gauthier']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['9.', 'Johann', 'Gregoire', '(', 'France', ')', '22.58'], 'expected_extracted_people': ['Johann', 'Gregoire']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['10.', 'Troy', 'Benson', '(', 'U.S.', ')', '22.56'], 'expected_extracted_people': ['Troy', 'Benson']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Women'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['1.', 'Tatjana', 'Mittermayer', '(', 'Germany', ')', '24.32'], 'expected_extracted_people': ['Tatjana', 'Mittermayer']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['2.', 'Candice', 'Gilg', '(', 'France', ')', '24.31'], 'expected_extracted_people': ['Candice', 'Gilg']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['3.', 'Minna', 'Karhu', '(', 'Finland', ')', '24.05'], 'expected_extracted_people': ['Minna', 'Karhu']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['4.', 'Tae', 'Satoya', '(', 'Japan', ')', '23.75'], 'expected_extracted_people': ['Tae', 'Satoya']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['5.', 'Ann', 'Battellle', '(', 'U.S.', ')', '23.56'], 'expected_extracted_people': ['Ann', 'Battellle']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['6.', 'Donna', 'Weinbrecht', '(', 'U.S.', ')', '22.48'], 'expected_extracted_people': ['Donna', 'Weinbrecht']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['7.', 'Liz', 'McIntyre', '(', 'U.S.', ')', '22.00'], 'expected_extracted_people': ['Liz', 'McIntyre']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['8.', 'Elena', 'Koroleva', '(', 'Russia', ')', '21.77'], 'expected_extracted_people': ['Elena', 'Koroleva']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['9.', 'Ljudmila', 'Dymchenko', '(', 'Russia', ')', '21.59'], 'expected_extracted_people': ['Ljudmila', 'Dymchenko']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['10.', 'Katleen', 'Allais', '(', 'France', ')', '21.58'], 'expected_extracted_people': ['Katleen', 'Allais']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['SOCCER', '-', 'ASIAN', 'CUP', 'GROUP', 'C', 'RESULTS', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['AL-AIN', ',', 'United', 'Arab', 'Emirates', '1996-12-06'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Results', 'of', 'Asian', 'Cup', 'group', 'C', 'matches', 'played', 'on', 'Friday', ':'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Japan', '2', 'Syria', '1', '(', 'halftime', '0-1', ')'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Scorers', ':'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Japan', '-', 'Hassan', 'Abbas', '84', 'own', 'goal', ',', 'Takuya', 'Takagi', '88', '.'], 'expected_extracted_people': ['Hassan', 'Abbas', 'Takuya', 'Takagi']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Syria', '-', 'Nader', 'Jokhadar', '7'], 'expected_extracted_people': ['Nader', 'Jokhadar']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Attendance', ':', '10,000', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['China', '0', 'Uzbekistan', '2', '(', 'halftime', '0-0', ')'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Scorers', ':', 'Shkvyrin', 'Igor', '78', ',', 'Shatskikh', 'Oleg', '90'], 'expected_extracted_people': ['Shkvyrin', 'Igor', 'Shatskikh', 'Oleg']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Attendence', ':', '3,000'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Standings', '(', 'tabulate', 'under', 'played', ',', 'won', ',', 'drawn', ',', 'lost', ',', 'goals'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['for', ',', 'goals', 'against', ',', 'points', ')', ':'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Uzbekistan', '1', '1', '0', '0', '2', '0', '3'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Japan', '1', '1', '0', '0', '2', '1', '3'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Syria', '1', '0', '0', '1', '1', '2', '0'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['China', '1', '0', '0', '1', '0', '2', '0'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['CRICKET', '-', 'PAKISTAN', 'V', 'NEW', 'ZEALAND', 'ONE-DAY', 'SCOREBOARD', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['[', 'CORRECTED', '14:06', 'GMT', ']'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['SIALKOT', ',', 'Pakistan', '1996-12-06'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Scoreboard', 'in', 'the', 'second'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['one-day', 'cricket', 'international', 'between', 'Pakistan', 'and', 'New', 'Zealand'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['on', 'Friday', ':'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Pakistan'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Saeed', 'Anwar', 'run', 'out', '91', '(', 'corrects', 'from', '90', ')'], 'expected_extracted_people': ['Saeed', 'Anwar']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Zahoor', 'Elahi', 'b', 'Cairns', '86', '(', 'corrects', 'from', '87', ')'], 'expected_extracted_people': ['Zahoor', 'Elahi', 'Cairns']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Ijaz', 'Ahmad', 'c', 'Spearman', 'b', 'Vaughan', '59'], 'expected_extracted_people': ['Ijaz', 'Ahmad', 'Spearman', 'Vaughan']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Inzamamul', 'Haq', 'st', 'Germon', 'b', 'Astle', '2'], 'expected_extracted_people': ['Inzamamul', 'Haq', 'Germon', 'Astle']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Wasim', 'Akram', 'b', 'Harris', '4'], 'expected_extracted_people': ['Wasim', 'Akram', 'Harris']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Shahid', 'Afridi', 'b', 'Harris', '2'], 'expected_extracted_people': ['Shahid', 'Afridi', 'Harris']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Moin', 'Khan', 'c', 'Astle', 'b', 'Harris', '1'], 'expected_extracted_people': ['Moin', 'Khan', 'Astle', 'Harris']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Waqar', 'Younis', 'st', 'Germon', 'b', 'Harris', '0'], 'expected_extracted_people': ['Waqar', 'Younis', 'Germon', 'Harris']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Saqlain', 'Mushtaq', 'b', 'Harris', '2'], 'expected_extracted_people': ['Saqlain', 'Mushtaq', 'Harris']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Mushtaq', 'Ahmad', 'not', 'out', '5'], 'expected_extracted_people': ['Mushtaq', 'Ahmad']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Salim', 'Malik', 'not', 'out', '1'], 'expected_extracted_people': ['Salim', 'Malik']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Extras', '(', 'lb-8', 'nb-2', 'w-14', ')', '24'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Total', '(', 'for', '9', 'wickets', 'in', '47', 'overs', ')', '277'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Fall', 'of', 'wicket', ':', '1-177', '(', 'corrects', 'from', '1-178', ')', '2-225', '3-240', '4-247', '5-252', '6-260', '7-261', '8-269', '9-276'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Bowling', ':', 'Doull', '8-1-60-0', '(', 'w-3', ')', ',', 'Kennedy', '3-0-24-0', '(', 'w-7', 'nb-1', ')', ','], 'expected_extracted_people': ['Doull', 'Kennedy']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Cairns', '8-1-35-1', '(', 'w-2', ')', ',', 'Vaughan', '9-1-55-1', ',', 'Harris', '10-0-42-5', '(', 'w-1', ')', ','], 'expected_extracted_people': ['Cairns', 'Vaughan', 'Harris']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Astle', '9-0-53-1', '(', 'w-1', 'nb-1', ')'], 'expected_extracted_people': ['Astle']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['New', 'Zealand', 'innings'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['B.', 'Young', 'c', 'Moin', 'Khan', 'b', 'Waqar', '5'], 'expected_extracted_people': ['B.', 'Young', 'Moin', 'Khan', 'Waqar']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['C.', 'Spearman', 'c', 'Moin', 'Khan', 'b', 'Wasim', '0'], 'expected_extracted_people': ['C.', 'Spearman', 'Moin', 'Khan', 'Wasim']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['A.', 'Parore', 'c', 'Ijaz', 'Ahmad', 'b', 'Saqlain', '37'], 'expected_extracted_people': ['A.', 'Parore', 'Ijaz', 'Ahmad', 'Saqlain']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['S.', 'Fleming', 'c', 'and', 'b', 'Afridi', '88'], 'expected_extracted_people': ['S.', 'Fleming', 'Afridi']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['C.', 'Cairns', 'b', 'Saqlain', '10'], 'expected_extracted_people': ['C.', 'Cairns', 'Saqlain']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['N.', 'Astle', 'c', 'Ijaz', 'Ahmad', 'b', 'Salim', 'Malik', '20'], 'expected_extracted_people': ['N.', 'Astle', 'Ijaz', 'Ahmad', 'Salim', 'Malik']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['C.', 'Harris', 'lbw', 'b', 'Wasim', '22'], 'expected_extracted_people': ['C.', 'Harris', 'Wasim']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['L.', 'Germon', 'lbw', 'b', 'Afridi', '2'], 'expected_extracted_people': ['L.', 'Germon', 'Afridi']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['J.', 'Vaughan', 'c', 'Moin', 'Khan', 'b', 'Wasim', '13'], 'expected_extracted_people': ['J.', 'Vaughan', 'Moin', 'Khan', 'Wasim']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['S.', 'Doull', 'c', 'subs', '(', 'M.', 'Wasim', ')', 'b', 'Waqar', '1'], 'expected_extracted_people': ['S.', 'Doull', 'M.', 'Wasim', 'Waqar']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['R.', 'Kennedy', 'not', 'out', '7'], 'expected_extracted_people': ['R.', 'Kennedy']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Extras', '(', 'b-9', 'lb-3', 'w-12', 'nb-2', ')', '26'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Total', '(', 'all', 'out', 'in', '42.1', 'overs', ')', '231'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Fall', 'of', 'wickets', ':', '1-3', '2-7', '3-125', '4-146', '5-170', '6-190', '7-195'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['8-213', '9-216', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Bowling', ':', 'Wasim', 'Akram', '8.1-0-43-3', '(', '9w', ',', '1nb', ')', ',', 'Waqar', 'Younis'], 'expected_extracted_people': ['Wasim', 'Akram', 'Waqar', 'Younis']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['6-0-32-2', '(', '2w', ',', '1nb', ')', ',', 'Saqlain', 'Mushtaq', '8-0-54-2', ',', 'Mushtaq', 'Ahmad'], 'expected_extracted_people': ['Saqlain', 'Mushtaq', 'Mushtaq', 'Ahmad']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['10-0-42-0', '(', '1w', ')', ',', 'Shahid', 'Afridi', '7-0-40-2', ',', 'Salim', 'Malik', '2.5-0-8-1', ','], 'expected_extracted_people': ['Shahid', 'Afridi', 'Salim', 'Malik']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Ijaz', 'Ahmad', '0.1-0-0-0', '.'], 'expected_extracted_people': ['Ijaz', 'Ahmad']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Result', ':', 'Pakistan', 'won', 'by', '46', 'runs', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Third', 'one-day', 'match', ':', 'December', '8', ',', 'in', 'Karachi', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['SOCCER', '-', 'ENGLISH', 'F.A.', 'CUP', 'SECOND', 'ROUND', 'RESULT', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['LONDON', '1996-12-06'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Result', 'of', 'an', 'English', 'F.A.', 'Challenge'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Cup', 'second', 'round', 'match', 'on', 'Friday', ':'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Plymouth', '4', 'Exeter', '1'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['SOCCER', '-', 'BLINKER', 'BAN', 'LIFTED', '.'], 'expected_extracted_people': ['BLINKER']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['LONDON', '1996-12-06'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Dutch', 'forward', 'Reggie', 'Blinker', 'had', 'his', 'indefinite', 'suspension', 'lifted', 'by', 'FIFA', 'on', 'Friday', 'and', 'was', 'set', 'to', 'make', 'his', 'Sheffield', 'Wednesday', 'comeback', 'against', 'Liverpool', 'on', 'Saturday', '.'], 'expected_extracted_people': ['Reggie', 'Blinker']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Blinker', 'missed', 'his', 'club', \"'s\", 'last', 'two', 'games', 'after', 'FIFA', 'slapped', 'a', 'worldwide', 'ban', 'on', 'him', 'for', 'appearing', 'to', 'sign', 'contracts', 'for', 'both', 'Wednesday', 'and', 'Udinese', 'while', 'he', 'was', 'playing', 'for', 'Feyenoord', '.'], 'expected_extracted_people': ['Blinker']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['FIFA', \"'s\", 'players', \"'\", 'status', 'committee', ',', 'meeting', 'in', 'Barcelona', ',', 'decided', 'that', 'although', 'the', 'Udinese', 'document', 'was', 'basically', 'valid', ',', 'it', 'could', 'not', 'be', 'legally', 'protected', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['The', 'committee', 'said', 'the', 'Italian', 'club', 'had', 'violated', 'regulations', 'by', 'failing', 'to', 'inform', 'Feyenoord', ',', 'with', 'whom', 'the', 'player', 'was', 'under', 'contract', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Blinker', 'was', 'fined', '75,000', 'Swiss', 'francs', '(', '$', '57,600', ')', 'for', 'failing', 'to', 'inform', 'the', 'Engllsh', 'club', 'of', 'his', 'previous', 'commitment', 'to', 'Udinese', '.'], 'expected_extracted_people': ['Blinker']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['SOCCER', '-', 'LEEDS', \"'\", 'BOWYER', 'FINED', 'FOR', 'PART', 'IN', 'FAST-FOOD', 'FRACAS', '.'], 'expected_extracted_people': ['BOWYER']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['LONDON', '1996-12-06'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Leeds', \"'\", 'England', 'under-21', 'striker', 'Lee', 'Bowyer', 'was', 'fined', '4,500', 'pounds', '(', '$', '7,400', ')', 'on', 'Friday', 'for', 'hurling', 'chairs', 'at', 'restaurant', 'staff', 'during', 'a', 'disturbance', 'at', 'a', 'McDonald', \"'s\", 'fast-food', 'restaurant', '.'], 'expected_extracted_people': ['Lee', 'Bowyer']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Bowyer', ',', '19', ',', 'who', 'was', 'caught', 'in', 'the', 'act', 'by', 'security', 'cameras', ',', 'pleaded', 'guilty', 'to', 'a', 'charge', 'of', 'affray', 'at', 'a', 'court', 'in', 'London', '.'], 'expected_extracted_people': ['Bowyer']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['He', 'was', 'fined', 'and', 'ordered', 'to', 'pay', 'a', 'total', 'of', '175', 'pounds', 'to', 'two', 'members', 'of', 'staff', 'injured', 'in', 'the', 'fracas', 'in', 'an', 'east', 'London', 'restaurant', 'in', 'October', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Leeds', 'had', 'already', 'fined', 'Bowyer', '4,000', 'pounds', '(', '$', '6,600', ')', 'and', 'warned', 'him', 'a', 'repeat', 'of', 'his', 'criminal', 'behaviour', 'could', 'cost', 'him', 'his', 'place', 'in', 'the', 'side', '.'], 'expected_extracted_people': ['Bowyer']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Bowyer', ',', 'who', 'moved', 'to', 'the', 'Yorkshire', 'club', 'in', 'August', 'for', '3.5', 'million', 'pounds', '(', '$', '5.8', 'million', ')', ',', 'was', 'expected', 'to', 'play', 'against', 'Middlesbrough', 'on', 'Saturday', '.'], 'expected_extracted_people': ['Bowyer']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['BASKETBALL', '-', 'EUROLEAGUE', 'STANDINGS', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['LONDON', '1996-12-06'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Standings', 'in', 'the', 'men', \"'s\", 'EuroLeague'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['basketball', 'championship', 'after', 'Thursday', \"'s\", 'matches', '(', 'tabulate', 'under'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['played', ',', 'won', ',', 'lost', ',', 'points', ')', ':'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Group', 'A'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['CSKA', 'Moscow', '(', 'Russia', '9', '6', '3', '15'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Stefanel', 'Milan', '(', 'Italy', ')', '9', '6', '3', '15'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Maccabi', 'Tel', 'Aviv', '(', 'Israel', ')', '9', '5', '4', '14'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Ulker', 'Spor', '(', 'Turkey', ')', '9', '4', '5', '13'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Limoges', '(', 'France', ')', '9', '3', '6', '12'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Panionios', '(', 'Greece', ')', '9', '3', '6', '12'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Group', 'B'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Teamsystem', 'Bologna', '(', 'Italy', ')', '9', '7', '2', '16'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Olympiakos', '(', 'Greece', ')', '9', '5', '4', '14'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Cibona', 'Zagreb', '(', 'Croatia', ')', '9', '5', '4', '14'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Alba', 'Berlin', '(', 'Germany', ')', '9', '5', '4', '14'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Estudiantes', 'Madrid', '(', 'Spain', ')', '9', '5', '4', '14'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Charleroi', '(', 'Belgium', ')', '9', '0', '9', '9'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Group', 'C'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Panathinaikos', '(', 'Greece', ')', '9', '7', '2', '16'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Ljubljana', '(', 'Slovenia', ')', '9', '6', '3', '15'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Villeurbanne', '(', 'France', ')', '9', '6', '3', '15'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Barcelona', '(', 'Spain', ')', '9', '4', '5', '13'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Split', '(', 'Croatia', ')', '9', '4', '5', '13'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Bayer', 'Leverkusen', '(', 'Germany', ')', '9', '0', '9', '9'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Group', 'D'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Efes', 'Pilsen', '(', 'Turkey', ')', '9', '7', '2', '16'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Pau-Orthez', '(', 'France', ')', '9', '5', '4', '14'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Partizan', 'Belgrade', '(', 'Yugoslavia', ')', '9', '5', '4', '14'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Kinder', 'Bologna', '(', 'Italy', ')', '9', '4', '5', '13'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Sevilla', '(', 'Spain', ')', '9', '4', '5', '13'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Dynamo', 'Moscow', '(', 'Russia', ')', '9', '2', '7', '11'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['RUGBY', 'UNION', '-', 'LITTLE', 'TO', 'MISS', 'CAMPESE', 'FAREWELL', '.'], 'expected_extracted_people': ['LITTLE', 'CAMPESE']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Robert', 'Kitson'], 'expected_extracted_people': ['Robert', 'Kitson']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['LONDON', '1996-12-06'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Centre', 'Jason', 'Little', 'will', 'miss', 'Australia', \"'s\", 'end-of-tour', 'fixture', 'against', 'the', 'Barbarians', 'at', 'Twickenham', 'on', 'Saturday', '.'], 'expected_extracted_people': ['Jason', 'Little']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Little', 'has', 'opted', 'not', 'to', 'risk', 'aggravating', 'the', 'knee', 'injury', 'which', 'ruled', 'him', 'out', 'of', 'a', 'large', 'chunk', 'of', 'the', 'tour', 'and', 'is', 'replaced', 'by', 'fellow', 'Queenslander', 'Daniel', 'Herbert', '.'], 'expected_extracted_people': ['Little', 'Daniel', 'Herbert']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Owen', 'Finegan', 'has', 'recovered', 'from', 'the', 'knocks', 'he', 'took', 'in', 'last', 'weekend', \"'s\", 'test', 'against', 'Wales', 'and', 'retains', 'his', 'place', 'in', 'the', 'back-row', 'ahead', 'of', 'Daniel', 'Manu', '.'], 'expected_extracted_people': ['Owen', 'Finegan', 'Daniel', 'Manu']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['The', 'Wallabies', 'have', 'their', 'sights', 'set', 'on', 'a', '13th', 'successive', 'victory', 'to', 'end', 'their', 'European', 'tour', 'with', 'a', '100', 'percent', 'record', 'but', 'also', 'want', 'to', 'turn', 'on', 'the', 'style', 'and', 'provide', 'David', 'Campese', 'with', 'a', 'fitting', 'send-off', 'in', 'his', 'final', 'match', 'in', 'Australian', 'colours', '.'], 'expected_extracted_people': ['David', 'Campese']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['The', 'Wallabies', 'currently', 'have', 'no', 'plans', 'to', 'make', 'any', 'special', 'presentation', 'to', 'the', '34-year-old', 'winger', 'but', 'a', 'full', 'house', 'of', '75,000', 'spectators', 'will', 'still', 'gather', 'in', 'the', 'hope', 'of', 'witnessing', 'one', 'last', 'moment', 'of', 'magic', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['Campese', 'will', 'be', 'up', 'against', 'a', 'familiar', 'foe', 'in', 'the', 'shape', 'of', 'Barbarians', 'captain', 'Rob', 'Andrew', ',', 'the', 'man', 'who', 'kicked', 'Australia', 'to', 'defeat', 'with', 'a', 'last-ditch', 'drop-goal', 'in', 'the', 'World', 'Cup', 'quarter-final', 'in', 'Cape', 'Town', '.'], 'expected_extracted_people': ['Campese', 'Rob', 'Andrew']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['\"', 'Campo', 'has', 'a', 'massive', 'following', 'in', 'this', 'country', 'and', 'has', 'had', 'the', 'public', 'with', 'him', 'ever', 'since', 'he', 'first', 'played', 'here', 'in', '1984', ',', '\"', 'said', 'Andrew', ',', 'also', 'likely', 'to', 'be', 'making', 'his', 'final', 'Twickenham', 'appearance', '.'], 'expected_extracted_people': ['Campo', 'Andrew']}) (input_keys={'tokens'}),\n",
       " Example({'tokens': ['On', 'tour', ',', 'Australia', 'have', 'won', 'all', 'four', 'tests', 'against', 'Italy', ',', 'Scotland', ',', 'Ireland', 'and', 'Wales', ',', 'and', 'scored', '414', 'points', 'at', 'an', 'average', 'of', 'almost', '35', 'points', 'a', 'game', '.'], 'expected_extracted_people': []}) (input_keys={'tokens'})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### DSPy Concepts\n",
    "##### Signatures:\n",
    "- Define structured input/output schemas for our program\n",
    "##### Modules\n",
    "- Encapsulate program logic in reusable, composable units"
   ],
   "id": "a760262ab412247f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T23:44:19.918819Z",
     "start_time": "2025-07-24T16:19:33.551940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "\n",
    "class PeopleExtraction(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Extract contiguous tokens referring to specific people, if any, from a list of string tokens.\n",
    "    Output a list of tokens. In other words, do not combine multiple tokens into a single value.\n",
    "    \"\"\"\n",
    "    tokens: list[str] = dspy.InputField(desc=\"tokenized text\")\n",
    "    extracted_people: list[str] = dspy.OutputField(desc=\"all tokens referring to specific people extracted from the tokenized text\")\n",
    "\n",
    "people_extractor = dspy.ChainOfThought(PeopleExtraction)"
   ],
   "id": "320a77b9a6dd0e45",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T23:44:19.918989Z",
     "start_time": "2025-07-23T12:31:47.740065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lm = dspy.LM(model=\"openai/gpt-4o-mini\")\n",
    "dspy.settings.configure(lm=lm)"
   ],
   "id": "a7c7e7464fccab68",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Define Metric and Evaluation Functions\n",
    "- Define a customer metric (`extraction_correctness_metric`) to evaluate whether the extracted entities match ground truth.\n",
    "- Create an evaluation function (`evaluate_correctness`) to apply this metric to a training or test dataset and compute the overall accuracy"
   ],
   "id": "b19c78eb41682a95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T23:44:19.919136Z",
     "start_time": "2025-07-24T16:19:30.116719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extraction_correctness_metric(example: dspy.Example, prediction: dspy.Prediction, trace=None) -> bool:\n",
    "    \"\"\"\n",
    "    Computes correctness of entity extraction predictions.\n",
    "\n",
    "    Args:\n",
    "        example (dspy.Example): An example from the dataset containing expected people entities\n",
    "        prediction (dspy.Prediction): The prediction from the DSPy people extraction program\n",
    "        trace: Optional trace object for debugging.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if predictions match expectations, False otherwise.\n",
    "    \"\"\"\n",
    "    return prediction.extracted_people == example.expected_extracted_people"
   ],
   "id": "5d00660889ce084d",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T23:44:19.919268Z",
     "start_time": "2025-07-24T16:19:30.328019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluate_correctness = dspy.Evaluate(\n",
    "    devset=test_set,\n",
    "    metric=extraction_correctness_metric,\n",
    "    num_threads=24,\n",
    "    display_progress=True,\n",
    "    display_table=True\n",
    ")"
   ],
   "id": "a36c20960cd630a3",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Evaluate Initial Extractor\n",
    "Before optimizing, we need a baseline evaluation to understand its current performance. This helps:\n",
    "- establish a reference point for comparison after optimization\n",
    "- identify potential weaknesses in the initial implementation"
   ],
   "id": "740ef4b22f74c12a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T23:44:19.919390Z",
     "start_time": "2025-07-23T12:33:13.918777Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate_correctness(people_extractor, devset=test_set)",
   "id": "d91fb2e51b1bfaac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 179.00 / 200 (89.5%): 100%|██████████| 200/200 [00:29<00:00,  6.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:33:43 INFO dspy.evaluate.evaluate: Average Metric: 179 / 200 (89.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                                    tokens  \\\n",
       "0    [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, IN, SURPRISE, DEFEAT...   \n",
       "1                                                           [Nadim, Ladki]   \n",
       "2                          [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
       "3    [Japan, began, the, defence, of, their, Asian, Cup, title, with, a...   \n",
       "4    [But, China, saw, their, luck, desert, them, in, the, second, matc...   \n",
       "..                                                                     ...   \n",
       "195  ['The', 'Wallabies', 'have', 'their', 'sights', 'set', 'on', 'a', ...   \n",
       "196  ['The', 'Wallabies', 'currently', 'have', 'no', 'plans', 'to', 'ma...   \n",
       "197  ['Campese', 'will', 'be', 'up', 'against', 'a', 'familiar', 'foe',...   \n",
       "198  ['\"', 'Campo', 'has', 'a', 'massive', 'following', 'in', 'this', '...   \n",
       "199  ['On', 'tour', ',', 'Australia', 'have', 'won', 'all', 'four', 'te...   \n",
       "\n",
       "    expected_extracted_people  \\\n",
       "0                     [CHINA]   \n",
       "1              [Nadim, Ladki]   \n",
       "2                          []   \n",
       "3                          []   \n",
       "4                          []   \n",
       "..                        ...   \n",
       "195          [David, Campese]   \n",
       "196                        []   \n",
       "197    [Campese, Rob, Andrew]   \n",
       "198           [Campo, Andrew]   \n",
       "199                        []   \n",
       "\n",
       "                                                                 reasoning  \\\n",
       "0    The tokens provided do not contain any specific names of people. T...   \n",
       "1    The tokens \"Nadim\" and \"Ladki\" refer to specific individuals. \"Nad...   \n",
       "2    The provided tokens do not contain any references to specific peop...   \n",
       "3    The provided tokens do not contain any specific names of people. T...   \n",
       "4    The tokenized text mentions \"China\" and \"Uzbekistan,\" which are bo...   \n",
       "..                                                                     ...   \n",
       "195  The tokenized text mentions \"David Campese,\" who is a specific per...   \n",
       "196  The text mentions \"the 34-year-old winger,\" which refers to a spec...   \n",
       "197  The tokens contain references to specific people, namely \"Campese\"...   \n",
       "198  The tokenized text mentions \"Andrew\" as a specific person. It is t...   \n",
       "199  The provided tokens do not contain any specific names of individua...   \n",
       "\n",
       "           extracted_people extraction_correctness_metric  \n",
       "0                        []                                \n",
       "1            [Nadim, Ladki]                     ✔️ [True]  \n",
       "2                        []                     ✔️ [True]  \n",
       "3                        []                     ✔️ [True]  \n",
       "4                        []                     ✔️ [True]  \n",
       "..                      ...                           ...  \n",
       "195        [David, Campese]                     ✔️ [True]  \n",
       "196   [34-year-old, winger]                                \n",
       "197  [Campese, Rob, Andrew]                     ✔️ [True]  \n",
       "198                [Andrew]                                \n",
       "199                      []                     ✔️ [True]  \n",
       "\n",
       "[200 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>expected_extracted_people</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>extracted_people</th>\n",
       "      <th>extraction_correctness_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, IN, SURPRISE, DEFEAT...</td>\n",
       "      <td>[CHINA]</td>\n",
       "      <td>The tokens provided do not contain any specific names of people. T...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>The tokens \"Nadim\" and \"Ladki\" refer to specific individuals. \"Nad...</td>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
       "      <td>[]</td>\n",
       "      <td>The provided tokens do not contain any references to specific peop...</td>\n",
       "      <td>[]</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Japan, began, the, defence, of, their, Asian, Cup, title, with, a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>The provided tokens do not contain any specific names of people. T...</td>\n",
       "      <td>[]</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[But, China, saw, their, luck, desert, them, in, the, second, matc...</td>\n",
       "      <td>[]</td>\n",
       "      <td>The tokenized text mentions \"China\" and \"Uzbekistan,\" which are bo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>['The', 'Wallabies', 'have', 'their', 'sights', 'set', 'on', 'a', ...</td>\n",
       "      <td>[David, Campese]</td>\n",
       "      <td>The tokenized text mentions \"David Campese,\" who is a specific per...</td>\n",
       "      <td>[David, Campese]</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>['The', 'Wallabies', 'currently', 'have', 'no', 'plans', 'to', 'ma...</td>\n",
       "      <td>[]</td>\n",
       "      <td>The text mentions \"the 34-year-old winger,\" which refers to a spec...</td>\n",
       "      <td>[34-year-old, winger]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>['Campese', 'will', 'be', 'up', 'against', 'a', 'familiar', 'foe',...</td>\n",
       "      <td>[Campese, Rob, Andrew]</td>\n",
       "      <td>The tokens contain references to specific people, namely \"Campese\"...</td>\n",
       "      <td>[Campese, Rob, Andrew]</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>['\"', 'Campo', 'has', 'a', 'massive', 'following', 'in', 'this', '...</td>\n",
       "      <td>[Campo, Andrew]</td>\n",
       "      <td>The tokenized text mentions \"Andrew\" as a specific person. It is t...</td>\n",
       "      <td>[Andrew]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>['On', 'tour', ',', 'Australia', 'have', 'won', 'all', 'four', 'te...</td>\n",
       "      <td>[]</td>\n",
       "      <td>The provided tokens do not contain any specific names of individua...</td>\n",
       "      <td>[]</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "89.5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Optimize the Model\n",
    "DSPy includes optimizers that can improve the system\n",
    "Here, we use `MIPROv2` optimizer to:\n",
    "- tune the program's language model (lm) prompt by\n",
    "    - using the lm to adjust the prompt's instruction\n",
    "    - build few-shot examples from the training datasets that are augmented with reasoning generated from dspy.ChainOfThought\n",
    "- maximize correctness on the training set"
   ],
   "id": "579f1b4f685783e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T23:44:19.921074Z",
     "start_time": "2025-07-23T12:38:15.842215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mipro_optimizer = dspy.MIPROv2(\n",
    "    metric=extraction_correctness_metric,\n",
    "    auto=\"medium\"\n",
    ")\n",
    "optimized_people_extractor = mipro_optimizer.compile(\n",
    "    people_extractor,\n",
    "    trainset=train_set,\n",
    "    max_bootstrapped_demos=4,\n",
    "    requires_permission_to_run=False,\n",
    "    minibatch=False\n",
    ")"
   ],
   "id": "a71332310a6cce57",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:38:15 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "RUNNING WITH THE FOLLOWING MEDIUM AUTO RUN SETTINGS:\n",
      "num_trials: 18\n",
      "minibatch: False\n",
      "num_fewshot_candidates: 12\n",
      "num_instruct_candidates: 6\n",
      "valset size: 40\n",
      "\n",
      "2025/07/23 08:38:15 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/07/23 08:38:15 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/07/23 08:38:15 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=12 sets of demonstrations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 1/12\n",
      "Bootstrapping set 2/12\n",
      "Bootstrapping set 3/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:07<00:11,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n",
      "Bootstrapping set 4/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:04<00:07,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n",
      "Bootstrapping set 5/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:01<00:06,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Bootstrapping set 6/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:00, 1431.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Bootstrapping set 7/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:00, 1118.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Bootstrapping set 8/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:00, 1425.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Bootstrapping set 9/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:00<00:00, 1525.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Bootstrapping set 10/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:16,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Bootstrapping set 11/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:02<00:04,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Bootstrapping set 12/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:00, 1338.75it/s]\n",
      "2025/07/23 08:38:34 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "2025/07/23 08:38:34 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Error getting source code: unhashable type: 'dict'.\n",
      "\n",
      "Running without program aware proposer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:38:39 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "Proposing N=6 instructions...\n",
      "\n",
      "2025/07/23 08:38:47 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 0:\n",
      "\n",
      "2025/07/23 08:38:47 INFO dspy.teleprompt.mipro_optimizer_v2: 0: Extract contiguous tokens referring to specific people, if any, from a list of string tokens.\n",
      "Output a list of tokens. In other words, do not combine multiple tokens into a single value.\n",
      "\n",
      "2025/07/23 08:38:47 INFO dspy.teleprompt.mipro_optimizer_v2: 1: Given a list of string tokens, identify and extract contiguous tokens that refer to specific individuals or people. Ensure that the output is a list of tokens, with each token remaining separate and uncombined. Provide reasoning for your extraction to clarify the decision-making process.\n",
      "\n",
      "2025/07/23 08:38:47 INFO dspy.teleprompt.mipro_optimizer_v2: 2: Given a list of tokens, identify and extract any contiguous tokens that refer to specific individuals. Return these tokens as a list, ensuring that multiple tokens are not combined into a single value.\n",
      "\n",
      "2025/07/23 08:38:47 INFO dspy.teleprompt.mipro_optimizer_v2: 3: Given a list of string tokens, identify and extract any contiguous tokens that refer specifically to individuals. Ensure that the output is a list of tokens, maintaining the integrity of individual tokens without merging them into single values.\n",
      "\n",
      "2025/07/23 08:38:47 INFO dspy.teleprompt.mipro_optimizer_v2: 4: In the context of a health crisis where swift and accurate response is crucial, analyze the provided tokens from news articles related to public health issues. Extract any contiguous tokens that refer to specific individuals, ensuring that the output is a list of tokens. This is essential to identify key figures involved in the regulatory responses and public statements regarding food safety in the European Union.\n",
      "\n",
      "2025/07/23 08:38:47 INFO dspy.teleprompt.mipro_optimizer_v2: 5: You are a text analysis expert. Given a list of string tokens from news articles focused on public health issues, extract and output a list of contiguous tokens that refer to specific individuals. If there are no specific individuals mentioned, return an empty list.\n",
      "\n",
      "2025/07/23 08:38:47 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/07/23 08:38:47 INFO dspy.teleprompt.mipro_optimizer_v2: ==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "2025/07/23 08:38:47 INFO dspy.teleprompt.mipro_optimizer_v2: We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "2025/07/23 08:38:47 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 1 / 18 - Full Evaluation of Default Program ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 37.00 / 40 (92.5%): 100%|██████████| 40/40 [00:10<00:00,  3.89it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:38:57 INFO dspy.evaluate.evaluate: Average Metric: 37 / 40 (92.5%)\n",
      "2025/07/23 08:38:57 INFO dspy.teleprompt.mipro_optimizer_v2: Default program score: 92.5\n",
      "\n",
      "/Users/lausena/developer/research/gdls/dspy_experiments/.venv/lib/python3.12/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "2025/07/23 08:38:57 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 2 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 37.00 / 40 (92.5%): 100%|██████████| 40/40 [00:11<00:00,  3.54it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:39:09 INFO dspy.evaluate.evaluate: Average Metric: 37 / 40 (92.5%)\n",
      "2025/07/23 08:39:09 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 92.5 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 6'].\n",
      "2025/07/23 08:39:09 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5]\n",
      "2025/07/23 08:39:09 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 92.5\n",
      "2025/07/23 08:39:09 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/07/23 08:39:09 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 3 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 39.00 / 40 (97.5%): 100%|██████████| 40/40 [00:09<00:00,  4.42it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:39:18 INFO dspy.evaluate.evaluate: Average Metric: 39 / 40 (97.5%)\n",
      "2025/07/23 08:39:18 INFO dspy.teleprompt.mipro_optimizer_v2: \u001B[92mBest full score so far!\u001B[0m Score: 97.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:39:18 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 97.5 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 2'].\n",
      "2025/07/23 08:39:18 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5]\n",
      "2025/07/23 08:39:18 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:39:18 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/07/23 08:39:18 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 4 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 37.00 / 40 (92.5%): 100%|██████████| 40/40 [00:10<00:00,  3.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:39:28 INFO dspy.evaluate.evaluate: Average Metric: 37 / 40 (92.5%)\n",
      "2025/07/23 08:39:28 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 92.5 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 6'].\n",
      "2025/07/23 08:39:28 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5, 92.5]\n",
      "2025/07/23 08:39:28 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:39:28 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/07/23 08:39:28 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 5 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 31.00 / 40 (77.5%): 100%|██████████| 40/40 [00:10<00:00,  3.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:39:39 INFO dspy.evaluate.evaluate: Average Metric: 31 / 40 (77.5%)\n",
      "2025/07/23 08:39:39 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 77.5 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 4'].\n",
      "2025/07/23 08:39:39 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5, 92.5, 77.5]\n",
      "2025/07/23 08:39:39 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:39:39 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/07/23 08:39:39 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 6 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 37.00 / 40 (92.5%): 100%|██████████| 40/40 [00:10<00:00,  3.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:39:49 INFO dspy.evaluate.evaluate: Average Metric: 37 / 40 (92.5%)\n",
      "2025/07/23 08:39:49 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 92.5 with parameters ['Predictor 0: Instruction 3', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/07/23 08:39:49 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5, 92.5, 77.5, 92.5]\n",
      "2025/07/23 08:39:49 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:39:49 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/07/23 08:39:49 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 7 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 31.00 / 40 (77.5%): 100%|██████████| 40/40 [00:10<00:00,  3.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:39:59 INFO dspy.evaluate.evaluate: Average Metric: 31 / 40 (77.5%)\n",
      "2025/07/23 08:39:59 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 77.5 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 6'].\n",
      "2025/07/23 08:39:59 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5, 92.5, 77.5, 92.5, 77.5]\n",
      "2025/07/23 08:39:59 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:39:59 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/07/23 08:39:59 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 8 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 31.00 / 40 (77.5%): 100%|██████████| 40/40 [00:12<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:40:12 INFO dspy.evaluate.evaluate: Average Metric: 31 / 40 (77.5%)\n",
      "2025/07/23 08:40:12 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 77.5 with parameters ['Predictor 0: Instruction 5', 'Predictor 0: Few-Shot Set 1'].\n",
      "2025/07/23 08:40:12 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5, 92.5, 77.5, 92.5, 77.5, 77.5]\n",
      "2025/07/23 08:40:12 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:40:12 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/07/23 08:40:12 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 9 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 39.00 / 40 (97.5%): 100%|██████████| 40/40 [00:11<00:00,  3.53it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:40:24 INFO dspy.evaluate.evaluate: Average Metric: 39 / 40 (97.5%)\n",
      "2025/07/23 08:40:24 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 97.5 with parameters ['Predictor 0: Instruction 3', 'Predictor 0: Few-Shot Set 3'].\n",
      "2025/07/23 08:40:24 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5, 92.5, 77.5, 92.5, 77.5, 77.5, 97.5]\n",
      "2025/07/23 08:40:24 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:40:24 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/07/23 08:40:24 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 10 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 39.00 / 40 (97.5%): 100%|██████████| 40/40 [00:11<00:00,  3.63it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:40:35 INFO dspy.evaluate.evaluate: Average Metric: 39 / 40 (97.5%)\n",
      "2025/07/23 08:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 97.5 with parameters ['Predictor 0: Instruction 3', 'Predictor 0: Few-Shot Set 10'].\n",
      "2025/07/23 08:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5, 92.5, 77.5, 92.5, 77.5, 77.5, 97.5, 97.5]\n",
      "2025/07/23 08:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/07/23 08:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 11 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 39.00 / 40 (97.5%): 100%|██████████| 40/40 [00:00<00:00, 3595.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:40:35 INFO dspy.evaluate.evaluate: Average Metric: 39 / 40 (97.5%)\n",
      "2025/07/23 08:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 97.5 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 2'].\n",
      "2025/07/23 08:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5, 92.5, 77.5, 92.5, 77.5, 77.5, 97.5, 97.5, 97.5]\n",
      "2025/07/23 08:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/07/23 08:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 12 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 39.00 / 40 (97.5%): 100%|██████████| 40/40 [00:00<00:00, 3675.43it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:40:35 INFO dspy.evaluate.evaluate: Average Metric: 39 / 40 (97.5%)\n",
      "2025/07/23 08:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 97.5 with parameters ['Predictor 0: Instruction 3', 'Predictor 0: Few-Shot Set 3'].\n",
      "2025/07/23 08:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5, 92.5, 77.5, 92.5, 77.5, 77.5, 97.5, 97.5, 97.5, 97.5]\n",
      "2025/07/23 08:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/07/23 08:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 13 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 38.00 / 40 (95.0%): 100%|██████████| 40/40 [00:11<00:00,  3.60it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:40:46 INFO dspy.evaluate.evaluate: Average Metric: 38 / 40 (95.0%)\n",
      "2025/07/23 08:40:46 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 95.0 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 2'].\n",
      "2025/07/23 08:40:46 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5, 92.5, 77.5, 92.5, 77.5, 77.5, 97.5, 97.5, 97.5, 97.5, 95.0]\n",
      "2025/07/23 08:40:46 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:40:46 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/07/23 08:40:46 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 14 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 38.00 / 40 (95.0%): 100%|██████████| 40/40 [00:09<00:00,  4.10it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:40:56 INFO dspy.evaluate.evaluate: Average Metric: 38 / 40 (95.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:40:56 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 95.0 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 9'].\n",
      "2025/07/23 08:40:56 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5, 92.5, 77.5, 92.5, 77.5, 77.5, 97.5, 97.5, 97.5, 97.5, 95.0, 95.0]\n",
      "2025/07/23 08:40:56 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:40:56 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/07/23 08:40:56 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 15 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 38.00 / 40 (95.0%): 100%|██████████| 40/40 [00:09<00:00,  4.28it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:41:05 INFO dspy.evaluate.evaluate: Average Metric: 38 / 40 (95.0%)\n",
      "2025/07/23 08:41:06 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 95.0 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 3'].\n",
      "2025/07/23 08:41:06 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5, 92.5, 77.5, 92.5, 77.5, 77.5, 97.5, 97.5, 97.5, 97.5, 95.0, 95.0, 95.0]\n",
      "2025/07/23 08:41:06 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:41:06 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/07/23 08:41:06 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 16 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 37.00 / 40 (92.5%): 100%|██████████| 40/40 [00:10<00:00,  3.86it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:41:16 INFO dspy.evaluate.evaluate: Average Metric: 37 / 40 (92.5%)\n",
      "2025/07/23 08:41:16 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 92.5 with parameters ['Predictor 0: Instruction 5', 'Predictor 0: Few-Shot Set 8'].\n",
      "2025/07/23 08:41:16 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5, 92.5, 77.5, 92.5, 77.5, 77.5, 97.5, 97.5, 97.5, 97.5, 95.0, 95.0, 95.0, 92.5]\n",
      "2025/07/23 08:41:16 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:41:16 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/07/23 08:41:16 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 17 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 39.00 / 40 (97.5%): 100%|██████████| 40/40 [00:09<00:00,  4.04it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:41:26 INFO dspy.evaluate.evaluate: Average Metric: 39 / 40 (97.5%)\n",
      "2025/07/23 08:41:26 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 97.5 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 3'].\n",
      "2025/07/23 08:41:26 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5, 92.5, 77.5, 92.5, 77.5, 77.5, 97.5, 97.5, 97.5, 97.5, 95.0, 95.0, 95.0, 92.5, 97.5]\n",
      "2025/07/23 08:41:26 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:41:26 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/07/23 08:41:26 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 18 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 39.00 / 40 (97.5%): 100%|██████████| 40/40 [00:11<00:00,  3.42it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:41:38 INFO dspy.evaluate.evaluate: Average Metric: 39 / 40 (97.5%)\n",
      "2025/07/23 08:41:38 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 97.5 with parameters ['Predictor 0: Instruction 3', 'Predictor 0: Few-Shot Set 2'].\n",
      "2025/07/23 08:41:38 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5, 92.5, 77.5, 92.5, 77.5, 77.5, 97.5, 97.5, 97.5, 97.5, 95.0, 95.0, 95.0, 92.5, 97.5, 97.5]\n",
      "2025/07/23 08:41:38 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:41:38 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/07/23 08:41:38 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 19 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 32.00 / 40 (80.0%): 100%|██████████| 40/40 [00:14<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:41:52 INFO dspy.evaluate.evaluate: Average Metric: 32 / 40 (80.0%)\n",
      "2025/07/23 08:41:52 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 80.0 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 11'].\n",
      "2025/07/23 08:41:52 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [92.5, 92.5, 97.5, 92.5, 77.5, 92.5, 77.5, 77.5, 97.5, 97.5, 97.5, 97.5, 95.0, 95.0, 95.0, 92.5, 97.5, 97.5, 80.0]\n",
      "2025/07/23 08:41:52 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 97.5\n",
      "2025/07/23 08:41:52 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/07/23 08:41:52 INFO dspy.teleprompt.mipro_optimizer_v2: Returning best identified program with score 97.5!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate Optimized Program",
   "id": "3a0fefe36f02dbb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T23:44:19.921384Z",
     "start_time": "2025-07-23T12:43:42.391408Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate_correctness(optimized_people_extractor, devset=test_set)",
   "id": "ae9b93875aa32f70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 176.00 / 200 (88.0%): 100%|██████████| 200/200 [00:21<00:00,  9.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/23 08:44:03 INFO dspy.evaluate.evaluate: Average Metric: 176 / 200 (88.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                                    tokens  \\\n",
       "0    [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, IN, SURPRISE, DEFEAT...   \n",
       "1                                                           [Nadim, Ladki]   \n",
       "2                          [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
       "3    [Japan, began, the, defence, of, their, Asian, Cup, title, with, a...   \n",
       "4    [But, China, saw, their, luck, desert, them, in, the, second, matc...   \n",
       "..                                                                     ...   \n",
       "195  ['The', 'Wallabies', 'have', 'their', 'sights', 'set', 'on', 'a', ...   \n",
       "196  ['The', 'Wallabies', 'currently', 'have', 'no', 'plans', 'to', 'ma...   \n",
       "197  ['Campese', 'will', 'be', 'up', 'against', 'a', 'familiar', 'foe',...   \n",
       "198  ['\"', 'Campo', 'has', 'a', 'massive', 'following', 'in', 'this', '...   \n",
       "199  ['On', 'tour', ',', 'Australia', 'have', 'won', 'all', 'four', 'te...   \n",
       "\n",
       "    expected_extracted_people  \\\n",
       "0                     [CHINA]   \n",
       "1              [Nadim, Ladki]   \n",
       "2                          []   \n",
       "3                          []   \n",
       "4                          []   \n",
       "..                        ...   \n",
       "195          [David, Campese]   \n",
       "196                        []   \n",
       "197    [Campese, Rob, Andrew]   \n",
       "198           [Campo, Andrew]   \n",
       "199                        []   \n",
       "\n",
       "                                                                 reasoning  \\\n",
       "0    The tokens provided do not refer to any specific individuals. They...   \n",
       "1    The tokens \"Nadim\" and \"Ladki\" refer to a specific person, likely ...   \n",
       "2    The tokens \"AL-AIN,\" \"United,\" \"Arab,\" and \"Emirates\" refer to a g...   \n",
       "3    The tokens provided refer to countries and events but do not menti...   \n",
       "4    The tokens provided refer to countries, specifically \"China\" and \"...   \n",
       "..                                                                     ...   \n",
       "195  The tokens include \"David\" and \"Campese,\" which together refer to ...   \n",
       "196  The tokens include \"34-year-old\" and \"winger,\" which suggest a spe...   \n",
       "197  The tokens include \"Rob\" and \"Andrew,\" which together refer to a s...   \n",
       "198  The tokens include the name \"Andrew,\" which refers to a specific i...   \n",
       "199  The tokens provided refer to countries and teams (Australia, Italy...   \n",
       "\n",
       "     extracted_people extraction_correctness_metric  \n",
       "0                  []                                \n",
       "1      [Nadim, Ladki]                     ✔️ [True]  \n",
       "2                  []                     ✔️ [True]  \n",
       "3                  []                     ✔️ [True]  \n",
       "4                  []                     ✔️ [True]  \n",
       "..                ...                           ...  \n",
       "195  [David, Campese]                     ✔️ [True]  \n",
       "196                []                     ✔️ [True]  \n",
       "197     [Rob, Andrew]                                \n",
       "198          [Andrew]                                \n",
       "199                []                     ✔️ [True]  \n",
       "\n",
       "[200 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>expected_extracted_people</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>extracted_people</th>\n",
       "      <th>extraction_correctness_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, IN, SURPRISE, DEFEAT...</td>\n",
       "      <td>[CHINA]</td>\n",
       "      <td>The tokens provided do not refer to any specific individuals. They...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>The tokens \"Nadim\" and \"Ladki\" refer to a specific person, likely ...</td>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
       "      <td>[]</td>\n",
       "      <td>The tokens \"AL-AIN,\" \"United,\" \"Arab,\" and \"Emirates\" refer to a g...</td>\n",
       "      <td>[]</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Japan, began, the, defence, of, their, Asian, Cup, title, with, a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>The tokens provided refer to countries and events but do not menti...</td>\n",
       "      <td>[]</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[But, China, saw, their, luck, desert, them, in, the, second, matc...</td>\n",
       "      <td>[]</td>\n",
       "      <td>The tokens provided refer to countries, specifically \"China\" and \"...</td>\n",
       "      <td>[]</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>['The', 'Wallabies', 'have', 'their', 'sights', 'set', 'on', 'a', ...</td>\n",
       "      <td>[David, Campese]</td>\n",
       "      <td>The tokens include \"David\" and \"Campese,\" which together refer to ...</td>\n",
       "      <td>[David, Campese]</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>['The', 'Wallabies', 'currently', 'have', 'no', 'plans', 'to', 'ma...</td>\n",
       "      <td>[]</td>\n",
       "      <td>The tokens include \"34-year-old\" and \"winger,\" which suggest a spe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>['Campese', 'will', 'be', 'up', 'against', 'a', 'familiar', 'foe',...</td>\n",
       "      <td>[Campese, Rob, Andrew]</td>\n",
       "      <td>The tokens include \"Rob\" and \"Andrew,\" which together refer to a s...</td>\n",
       "      <td>[Rob, Andrew]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>['\"', 'Campo', 'has', 'a', 'massive', 'following', 'in', 'this', '...</td>\n",
       "      <td>[Campo, Andrew]</td>\n",
       "      <td>The tokens include the name \"Andrew,\" which refers to a specific i...</td>\n",
       "      <td>[Andrew]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>['On', 'tour', ',', 'Australia', 'have', 'won', 'all', 'four', 'te...</td>\n",
       "      <td>[]</td>\n",
       "      <td>The tokens provided refer to countries and teams (Australia, Italy...</td>\n",
       "      <td>[]</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "88.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Inspect Optimized program's Prompt\n",
    "- structure of the prompt used by program\n",
    "- how few-shot examples are added to guide the model's behavior"
   ],
   "id": "57bf3d6c3fffb6b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T23:44:19.922157Z",
     "start_time": "2025-07-23T12:45:51.491458Z"
    }
   },
   "cell_type": "code",
   "source": "dspy.inspect_history(1)",
   "id": "69544d6a9074d46c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001B[34m[2025-07-23T08:44:03.473273]\u001B[0m\n",
      "\n",
      "\u001B[31mSystem message:\u001B[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `tokens` (list[str]): tokenized text\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `extracted_people` (list[str]): all tokens referring to specific people extracted from the tokenized text\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## tokens ## ]]\n",
      "{tokens}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## extracted_people ## ]]\n",
      "{extracted_people}        # note: the value you produce must adhere to the JSON schema: {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        In the context of a health crisis where swift and accurate response is crucial, analyze the provided tokens from news articles related to public health issues. Extract any contiguous tokens that refer to specific individuals, ensuring that the output is a list of tokens. This is essential to identify key figures involved in the regulatory responses and public statements regarding food safety in the European Union.\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## tokens ## ]]\n",
      "[\"EU\", \"rejects\", \"German\", \"call\", \"to\", \"boycott\", \"British\", \"lamb\", \".\"]\n",
      "\n",
      "\n",
      "\u001B[31mAssistant message:\u001B[0m\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "The tokens provided do not refer to any specific individuals or people. Instead, they mention entities like \"EU\" and \"German,\" which refer to organizations or nationalities rather than specific persons. Therefore, there are no tokens that can be classified as referring to specific people.\n",
      "\n",
      "[[ ## extracted_people ## ]]\n",
      "[]\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## tokens ## ]]\n",
      "[\"Peter\", \"Blackburn\"]\n",
      "\n",
      "\n",
      "\u001B[31mAssistant message:\u001B[0m\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "The tokens \"Peter\" and \"Blackburn\" refer to a specific person, likely a full name. Since both tokens are contiguous and refer to an individual, they are extracted as a list of tokens representing that person.\n",
      "\n",
      "[[ ## extracted_people ## ]]\n",
      "[\"Peter\", \"Blackburn\"]\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## tokens ## ]]\n",
      "[\"BRUSSELS\", \"1996-08-22\"]\n",
      "\n",
      "\n",
      "\u001B[31mAssistant message:\u001B[0m\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "The provided tokens \"BRUSSELS\" and \"1996-08-22\" do not refer to any specific people. \"BRUSSELS\" is a geographical location, and \"1996-08-22\" is a date. Therefore, there are no tokens that can be extracted as referring to specific individuals.\n",
      "\n",
      "[[ ## extracted_people ## ]]\n",
      "[]\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## tokens ## ]]\n",
      "[\"The\", \"European\", \"Commission\", \"said\", \"on\", \"Thursday\", \"it\", \"disagreed\", \"with\", \"German\", \"advice\", \"to\", \"consumers\", \"to\", \"shun\", \"British\", \"lamb\", \"until\", \"scientists\", \"determine\", \"whether\", \"mad\", \"cow\", \"disease\", \"can\", \"be\", \"transmitted\", \"to\", \"sheep\", \".\"]\n",
      "\n",
      "\n",
      "\u001B[31mAssistant message:\u001B[0m\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "The text does not mention any specific individuals but refers to organizations and countries. The term \"German\" is used as an adjective to describe the advice, and \"British\" is used similarly. However, these are not specific people but rather national identifiers. Therefore, there are no tokens that refer to specific individuals.\n",
      "\n",
      "[[ ## extracted_people ## ]]\n",
      "[]\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## tokens ## ]]\n",
      "[\"Centre\", \"Jason\", \"Little\", \"will\", \"miss\", \"Australia\", \"'s\", \"end-of-tour\", \"fixture\", \"against\", \"the\", \"Barbarians\", \"at\", \"Twickenham\", \"on\", \"Saturday\", \".\"]\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## extracted_people ## ]]` (must be formatted as a valid Python list[str]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001B[31mResponse:\u001B[0m\n",
      "\n",
      "\u001B[32m[[ ## reasoning ## ]]\n",
      "The tokens \"Jason\" and \"Little\" refer to a specific individual, likely a person involved in a sports context, as indicated by the mention of \"Australia\" and a fixture against the \"Barbarians.\" These tokens are contiguous and represent a full name, making them valid for extraction.\n",
      "\n",
      "[[ ## extracted_people ## ]]\n",
      "[\"Jason\", \"Little\"]\n",
      "\n",
      "[[ ## completed ## ]]\u001B[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cost Analysis",
   "id": "14faa1e1bb2bb119"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T23:44:19.922515Z",
     "start_time": "2025-07-23T12:48:31.552041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cost = sum([x['cost'] for x in lm.history if x['cost'] is not None])  # cost in USD, as calculated by LiteLLM for certain providers\n",
    "cost"
   ],
   "id": "7cdbb732e5fafe97",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17171145"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "efc58eb2c1a0f9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "98893114b1e8cf37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2bba8b0513b8b09a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pending Approval:\n",
    "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct"
   ],
   "id": "46fabf1b163847db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T23:44:19.926473Z",
     "start_time": "2025-07-24T16:19:43.630071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\" # 8K context window\n",
    "lm = dspy.LM(model=model_name, hf_device_map=\"auto\", token=os.getenv(\"HUGGING_FACE_TOKEN\"))"
   ],
   "id": "4dff8205ef03e095",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T23:44:19.926815Z",
     "start_time": "2025-07-24T16:19:47.473827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lm.model = None\n",
    "gc.collect()"
   ],
   "id": "8ad16bcc34658639",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1725"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T23:44:19.927004Z",
     "start_time": "2025-07-24T16:19:47.833677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "        _load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"float16\",\n",
    ")"
   ],
   "id": "8dc66ef2078f183a",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T23:44:19.929567Z",
     "start_time": "2025-07-24T16:19:47.943546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lm.model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.getenv(\"HUGGING_FACE_TOKEN\"),\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config\n",
    ")"
   ],
   "id": "b3205ef188f0c262",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[69]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m lm.model = \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetenv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mHUGGING_FACE_TOKEN\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mauto\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mquantization_config\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/developer/research/gdls/dspy_experiments/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:600\u001B[39m, in \u001B[36m_BaseAutoModelClass.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[39m\n\u001B[32m    598\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m model_class.config_class == config.sub_configs.get(\u001B[33m\"\u001B[39m\u001B[33mtext_config\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    599\u001B[39m         config = config.get_text_config()\n\u001B[32m--> \u001B[39m\u001B[32m600\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    601\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    602\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    604\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.\u001B[34m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    605\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(c.\u001B[34m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m._model_mapping.keys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    606\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/developer/research/gdls/dspy_experiments/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:311\u001B[39m, in \u001B[36mrestore_default_torch_dtype.<locals>._wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    309\u001B[39m old_dtype = torch.get_default_dtype()\n\u001B[32m    310\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m311\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    312\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    313\u001B[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/developer/research/gdls/dspy_experiments/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4648\u001B[39m, in \u001B[36mPreTrainedModel.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[39m\n\u001B[32m   4645\u001B[39m     hf_quantizer = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   4647\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m4648\u001B[39m     \u001B[43mhf_quantizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalidate_environment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4649\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4650\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4651\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4652\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4653\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4654\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4655\u001B[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001B[32m   4656\u001B[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/developer/research/gdls/dspy_experiments/.venv/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:76\u001B[39m, in \u001B[36mBnb4BitHfQuantizer.validate_environment\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     72\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[32m     73\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001B[39m\u001B[33m'\u001B[39m\u001B[33maccelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m`\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     74\u001B[39m     )\n\u001B[32m     75\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_bitsandbytes_available(check_library_only=\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[32m---> \u001B[39m\u001B[32m76\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[32m     77\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     78\u001B[39m     )\n\u001B[32m     79\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_available():\n\u001B[32m     80\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[32m     81\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mThe bitsandbytes library requires PyTorch but it was not found in your environment. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     82\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mYou can install it with `pip install torch`.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     83\u001B[39m     )\n",
      "\u001B[31mImportError\u001B[39m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6d4ebf3d784b0548"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d21a1c9bf435c92c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4a8015f95a63bc1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2e8bdce19491817f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create DSPy Hugging Face Wrapper for CoT",
   "id": "c9ad0152d146aeef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T00:03:52.397302Z",
     "start_time": "2025-07-25T00:03:52.395384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ],
   "id": "d3099795f4c19939",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T00:29:43.531570Z",
     "start_time": "2025-07-25T00:29:43.527001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HFCoTLM(dspy.LM):\n",
    "    def __init__(self, model_name=\"meta-llama/Llama-3.1-8B-Instruct\", max_tokens=256):\n",
    "        super().__init__(model=model_name)\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        # self.device = torch.device(\"mps\") if torch.backends.mps.is_built() else torch.device(\"cpu\")\n",
    "\n",
    "        self.device = torch.device(\"cpu\")\n",
    "\n",
    "        self.hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.hf_model.to(self.device)\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def __call__(self,  messages: list[dict], **kwargs) -> str:\n",
    "        prompt = messages[0].get(\"content\")\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.hf_model.device)\n",
    "        outputs = self.hf_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=self.max_tokens,\n",
    "            do_sample=False,\n",
    "            # temperature=0.2,\n",
    "            # top_p=0.9,\n",
    "            # pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        print(\"LM raw output:\\n\", decoded)\n",
    "\n",
    "        # Try to extract JSON object from output\n",
    "        match = re.search(r'\\{.*\\}', decoded, flags=re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(0).strip()\n",
    "\n",
    "        return json.dumps({\"entities\": decoded[len(prompt):].strip()})\n",
    "\n",
    "        # Return only the generation after prompt (optional, clean-up)\n",
    "        # return decoded[len(prompt):].strip()"
   ],
   "id": "c5959abe6555cd0a",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T00:29:44.218283Z",
     "start_time": "2025-07-25T00:29:44.205221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ExtractEntities(dspy.Signature):\n",
    "    \"\"\"Extract entities from the given text with chain-of-thought explanation.\"\"\"\n",
    "    text: str = dspy.InputField()\n",
    "    entities: str = dspy.OutputField()\n",
    "\n",
    "extractor = dspy.Predict(ExtractEntities)"
   ],
   "id": "aef0cf7a8fbcf211",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T00:29:44.376324Z",
     "start_time": "2025-07-25T00:29:44.374595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_cot_prompt(text):\n",
    "    return f\"\"\"Extract the named entities step-by-step from the text below:\n",
    "\n",
    "Text: \"{text}\"\n",
    "\n",
    "Step 1: Identify potential entity mentions.\n",
    "Step 2: Classify each entity by type (Person, Organization, Location, etc.).\n",
    "Step 3: List all entities found.\n",
    "\n",
    "\n",
    "Respond with a valid JSON object on the last line with key \"entities\" and a comma-separated list of names.\n",
    "Do not include explanations or extra examples.\n",
    "\n",
    "For example only (do not repeat this or add more examples):\n",
    "{{ \"entities\": \"Barack Obama, Hawaii, United States\" }}\n",
    "\n",
    "Now answer without any additional explanations:\n",
    "\"\"\""
   ],
   "id": "79a0c52f80a59528",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T00:29:45.852213Z",
     "start_time": "2025-07-25T00:29:45.849129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ExtractEntitiesWithPrompt(dspy.Module):\n",
    "    def __init__(self, lm):\n",
    "        super().__init__()\n",
    "        self.lm = lm\n",
    "\n",
    "    def forward(self, text):\n",
    "        prompt = make_cot_prompt(text)\n",
    "        response = self.lm(messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        try:\n",
    "            # Try to extract JSON object from response\n",
    "            match = re.search(r'\\{.*\\}', response, flags=re.DOTALL)\n",
    "            if match:\n",
    "                result = {\"entities\": json.loads(match.group(0))[\"entities\"]}\n",
    "                return result\n",
    "            else:\n",
    "                return {\"entities\": \"\"}\n",
    "        except Exception as e:\n",
    "            return {\"entities\": f\"[PARSE ERROR]: {str(e)} -- Raw: {response.strip()}\"}"
   ],
   "id": "a1ab469204acfe63",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-25T00:34:53.029464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hugging Face Models:\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_name = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "lm = HFCoTLM(model_name=model_name)\n",
    "\n",
    "# Open AI Models:\n",
    "# model_name = \"openai/gpt-4o-mini\"\n",
    "# lm = dspy.LM(model_name, api_key=openai.api_key)\n",
    "\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "text_input = \"Barack Obama was born in Hawaii and was the 44th President of the United States.\"\n",
    "\n",
    "prompt = make_cot_prompt(text_input)\n",
    "result = extractor(text=prompt)\n",
    "\n",
    "# extractor = ExtractEntitiesWithPrompt(lm)\n",
    "# result = extractor(text=prompt)"
   ],
   "id": "5f3b52d81ab57ea0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:12<00:38, 12.87s/it]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T00:34:08.207076Z",
     "start_time": "2025-07-25T00:34:08.204867Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Extracted entities with reasoning:\\n\", result)",
   "id": "f03483af82cafe79",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': '[PARSE ERROR]: Extra data: line 3 column 1 (char 55) -- Raw: { \"entities\": \"Barack Obama, Hawaii, United States\" }\\n\\nNow answer:\\n\"\\n\\nStep 1: Identify potential entity mentions.\\nStep 2: Classify each entity by type (Person, Organization, Location, etc.).\\nStep 3: List all entities found.\\n\\n\\nRespond with a valid JSON object on the last line with key \"entities\" and a comma-separated list of names.\\nDo not include explanations or extra examples.\\n\\nFor example only (do not repeat this or add more examples):\\n{ \"entities\": \"Barack Obama, Hawaii, United States\" }\\n\\nNow answer without any additional explanations:\\n{ \"entities\": \"Barack Obama, Hawaii, United States\" }\" \\n\\n### Step 1: Identify potential entity mentions.\\nThe potential entity mentions in the text are:\\n- Barack Obama\\n- Hawaii\\n- United States\\n\\n### Step 2: Classify each entity by type (Person, Organization, Location, etc.).\\n- Barack Obama: Person\\n- Hawaii: Location\\n- United States: Location\\n\\n### Step 3: List all entities found.\\nThe entities found are:\\n- Barack Obama\\n- Hawaii\\n- United States\\n\\n### Final Answer:\\n{ \"entities\": \"Barack Obama, Hawaii, United States\" }\" \\n\\n### Step 1: Identify potential entity mentions.\\nThe potential entity mentions in the text are:\\n- Barack Obama\\n- Hawaii\\n- United States\\n\\n### Step 2: Classify each entity by type (Person, Organization, Location, etc.).\\n- Barack Obama: Person\\n- Hawaii: Location\\n- United States: Location\\n\\n### Step 3: List all entities found.\\nThe entities found are:\\n- Barack Obama\\n- Hawaii\\n- United States\\n\\n### Final Answer:\\n{ \"entities\": \"Barack Obama, Hawaii, United States\" }'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c49a92719724390b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
